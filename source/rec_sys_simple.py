# -*- coding: utf-8 -*-
"""rec_sys_simple.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P64VIbq6-FWVKYo503NT4y-_GgaIilgD

# Recommender System for Economic Diversification

Recommender system code inspired from https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb
"""

# show which version of TF working
!pip show tensorflow

# show which hversion of keras
!pip show keras

# check GPU connection
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# import libraries
import os
import warnings
import sys
import math
import matplotlib.pyplot as plt
import nltk
import numpy as np
import pandas as pd
import pickle
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns
from collections import Counter, defaultdict, OrderedDict
from itertools import chain
from nltk.tokenize import sent_tokenize, word_tokenize
from keras import optimizers, regularizers
from keras.callbacks import ModelCheckpoint
from keras.layers import advanced_activations, Concatenate, Dense, Dot, Dropout, Embedding, Flatten, Input, LSTM, Reshape
from keras.models import load_model, Model, Sequential
from keras.preprocessing.text import Tokenizer
from keras.utils import np_utils
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split

"""# ETL Pipeline

## Setup Google Drive + Get the Data
"""

# mount Google Drive locally
from google.colab import drive
drive.mount('/content/gdrive')

os.getcwd()

# upload data_17_0.h5 from Drive and preview
in_filename = '/content/gdrive/My Drive/Insight_AI/Insight_Net_Align/data/raw/data_17_0.h5'
hdf = pd.HDFStore(in_filename, mode='r')

# preview keys in HDF
hdf.keys()

# extract country summaries
df_country = hdf.get('/country')
print(df_country.shape)
df_country.head()

# extract hs product data
df2 = hdf.get('/country_hsproduct2digit_year')
df4 = hdf.get('/country_hsproduct4digit_year')
df6 = hdf.get('/country_hsproduct6digit_year')
print(df6.shape)
df6.head()

# clean the NaNs
df4 = df4.fillna(0)
df6 = df6.fillna(0)

# cut the data for only 2007, 2008, and 2017 analysis for df2
df2_2007 = df2[df2['year'] == 2007]
print(df2_2007.shape)
df2_2008 = df2[df2['year'] == 2008]
print(df2_2008.shape)
df2_2017 = df2[df2['year'] == 2017]
print(df2_2017.shape)

# cut the data for only 2007, 2008, and 2017 analysis for df4
df4_2007 = df4[df4['year'] == 2007]
print(df4_2007.shape)
df4_2008 = df4[df4['year'] == 2008]
print(df4_2008.shape)
df4_2017 = df4[df4['year'] == 2017]
print(df4_2017.shape)

# cut the data for only 2007, 2008, and 2017 analysis for df6
df6_2007 = df6[df6['year'] == 2007]
print(df6_2007.shape)
df6_2008 = df6[df6['year'] == 2008]
print(df6_2008.shape)
df6_2017 = df6[df6['year'] == 2017]
print(df6_2017.shape)

# check not null vals
'''
df6_2016_sparse = df6_2016[df6_2016['export_value'].notnull()]
print(df6_2016_sparse.shape)
df6_2017_sparse = df6_2017[df6_2017['export_value'].notnull()]
print(df6_2017_sparse.shape)

# impute NaN values with 0
df6_2016_clean = df6_2016.fillna(0)
print(df6_2016.shape)
df6_2017_clean = df6_2017.fillna(0)
print(df6_2017.shape)
df6_2017_clean.head()
'''

# extract country hs product lookbacks
df2_lookback = hdf.get('/country_hsproduct2digit_lookback')
df4_lookback = hdf.get('/country_hsproduct4digit_lookback') # there is no 6digit lookback
df4_lookback.head()

# extract country summaries lookback
df_country_lookback = hdf.get('/country_lookback')
df_country_lookback.tail()

# compare country + lookback shapes
print(df_country.shape)
print(df_country_lookback.shape)

# look at just one set of lookbacks for a given year
df_country_lookback3 = df_country_lookback[df_country_lookback['lookback_year'] == 2014]
print(df_country_lookback3.shape)

# extract classes - index is the product_id in other tables
df_classes = hdf.get('/classifications/hs_product')
type(df_classes)
print(df_classes.shape)
df2_classes = df_classes[df_classes['level'] == '2digit']
df4_classes = df_classes[df_classes['level'] == '4digit']
df6_classes = df_classes[df_classes['level'] == '6digit']

print(df6_classes.shape)
df6_classes.head()

# get locations - index is location_id in other tables
df_locations = hdf.get('/classifications/location')
print(df_locations.shape)
df_locations.head()



"""# Supervised Machine Learning Task

Treat this as a problem where for every (country, product) pair, we know the rating (export_value). Train on one year at a time.

## Build Training Set
"""

print(df2_2007.shape)
df2_2007.head()

# df2, for 2007
df2_2007_norm = df2_2007.copy(deep=True)

# using standard normalization - control for outliers
df2_2007_norm['export_value'] = (df2_2007['export_value']-df2_2007['export_value'].mean())/df2_2007['export_value'].std()
df2_2007_norm['import_value'] = (df2_2007['import_value']-df2_2007['import_value'].mean())/df2_2007['import_value'].std()
df2_2007_norm.head()

# using min-max normalization - goes between 0 and 1
df2_2007_norm['export_value'] = (df2_2007['export_value']-df2_2007['export_value'].min())/(df2_2007['export_value'].max()-df2_2007['export_value'].min())
df2_2007_norm['import_value'] = (df2_2007['import_value']-df2_2007['import_value'].min())/(df2_2007['import_value'].max()-df2_2007['import_value'].min())
df2_2007_norm.head()

# df2, for 2008
df2_2008_norm = df2_2008.copy(deep=True)

# using standard normalization - control for outliers
df2_2008_norm['export_value'] = (df2_2008['export_value']-df2_2008['export_value'].mean())/df2_2008['export_value'].std()
df2_2008_norm['import_value'] = (df2_2008['import_value']-df2_2008['import_value'].mean())/df2_2008['import_value'].std()
df2_2008_norm.head()

# using min-max normalization - goes between 0 and 1
df2_2008_norm['export_value'] = (df2_2008['export_value']-df2_2008['export_value'].min())/(df2_2008['export_value'].max()-df2_2008['export_value'].min())
df2_2008_norm['import_value'] = (df2_2008['import_value']-df2_2008['import_value'].min())/(df2_2008['import_value'].max()-df2_2008['import_value'].min())
df2_2008_norm.head()

# df4, for 2007
df4_2007_norm = df4_2007.copy(deep=True)

# using standard normalization - control for outliers
df4_2007_norm['export_value'] = (df4_2007['export_value']-df4_2007['export_value'].mean())/df4_2007['export_value'].std()
df4_2007_norm['import_value'] = (df4_2007['import_value']-df4_2007['import_value'].mean())/df4_2007['import_value'].std()
df4_2007_norm.head()

# using min-max normalization - goes between 0 and 1
df4_2007_norm['export_value'] = (df4_2007['export_value']-df4_2007['export_value'].min())/(df4_2007['export_value'].max()-df4_2007['export_value'].min())
df4_2007_norm['import_value'] = (df4_2007['import_value']-df4_2007['import_value'].min())/(df4_2007['import_value'].max()-df4_2007['import_value'].min())
df4_2007_norm.head()

# df4, for 2008
df4_2008_norm = df4_2008.copy(deep=True)

# using standard normalization - control for outliers
df4_2008_norm['export_value'] = (df4_2008['export_value']-df4_2008['export_value'].mean())/df4_2008['export_value'].std()
df4_2008_norm['import_value'] = (df4_2008['import_value']-df4_2008['import_value'].mean())/df4_2008['import_value'].std()
df4_2008_norm.head()

# using min-max normalization - goes between 0 and 1
df4_2008_norm['export_value'] = (df4_2008['export_value']-df4_2008['export_value'].min())/(df4_2008['export_value'].max()-df4_2008['export_value'].min())
df4_2008_norm['import_value'] = (df4_2008['import_value']-df4_2008['import_value'].min())/(df4_2008['import_value'].max()-df4_2008['import_value'].min())
df4_2008_norm.head()

# df6, for 2007
df6_2007_norm = df6_2007.copy(deep=True)

# using standard normalization - control for outliers
df6_2007_norm['export_value'] = (df6_2007['export_value']-df6_2007['export_value'].mean())/df6_2007['export_value'].std()
df6_2007_norm['import_value'] = (df6_2007['import_value']-df6_2007['import_value'].mean())/df6_2007['import_value'].std()
df6_2007_norm.head()

# using min-max normalization - goes between 0 and 1
df6_2007_norm['export_value'] = (df6_2007['export_value']-df6_2007['export_value'].min())/(df6_2007['export_value'].max()-df6_2007['export_value'].min())
df6_2007_norm['import_value'] = (df6_2007['import_value']-df6_2007['import_value'].min())/(df6_2007['import_value'].max()-df6_2007['import_value'].min())
df6_2007_norm.head()

# df6, for 2008
df6_2008_norm = df6_2008.copy(deep=True)

# using standard normalization - control for outliers
df6_2008_norm['export_value'] = (df6_2008['export_value']-df6_2008['export_value'].mean())/df6_2008['export_value'].std()
df6_2008_norm['import_value'] = (df6_2008['import_value']-df6_2008['import_value'].mean())/df6_2008['import_value'].std()
df6_2008_norm.head()

# using min-max normalization - goes between 0 and 1
df6_2008_norm['export_value'] = (df6_2008['export_value']-df6_2008['export_value'].min())/(df6_2008['export_value'].max()-df6_2008['export_value'].min())
df6_2008_norm['import_value'] = (df6_2008['import_value']-df6_2008['import_value'].min())/(df6_2008['import_value'].max()-df6_2008['import_value'].min())
df6_2008_norm.head()

'''
# build train, test split
train, test = train_test_split(df6_2016_clean, test_size=0.2, random_state=42)
'''

df6_2007_norm.describe()
df6_2008_norm.describe()

df6_2007.describe()
df6_2008.describe()

df6_2007_norm.dtypes
df6_2008_norm.dtypes

'''
test.head()
'''



'''
# normalize train ONLY - don't leak data from test
train_norm = train.copy(deep=True)
# using standard normalization - control for outliers
train_norm['export_value'] = (train['export_value']-train['export_value'].mean())/train['export_value'].std()
train_norm['import_value'] = (train['import_value']-train['import_value'].mean())/train['import_value'].std()
train_norm.head()

# using min-max normalization - goes between 0 and 1
train_norm['export_value'] = (train['export_value']-train['export_value'].min())/(train['export_value'].max()-train['export_value'].min())
train_norm['import_value'] = (train['import_value']-train['import_value'].min())/(train['import_value'].std()-train['import_value'].min())
train_norm.head()

# using quantile normalization

# implementation - https://stackoverflow.com/questions/37935920/quantile-normalization-on-pandas-dataframe
# idea - https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network
# theory - https://en.wikipedia.org/wiki/Quantile_normalization

# rank_mean = train_norm['export_value'].stack().groupby(train_norm['export_value]'.rank(method='first').stack().astype(int)).mean()
# train_norm['export_value'] = train_norm['export_value'].rank(method='min').stack().astype(int).map(rank_mean).unstack()
# train_norm.head()

# drop anything with export_value == 0
# train_norm = train_norm[train_norm['export_value'] != 0]
# train_norm.head()
'''

'''
# describe train_norm
train_norm.describe()
'''

'''
# check if any null values remain to be sure
len(train_norm[train_norm['export_value'].isnull()])
'''

# df2, for 2007
print(df2_2007_norm['export_value'].mean())
print(df2_2007_norm['export_value'].var())
df2_2007_norm['export_value'].hist(bins=10)

# df4, for 2007
print(df4_2007_norm['export_value'].mean())
print(df4_2007_norm['export_value'].var())
df4_2007_norm['export_value'].hist(bins=10)

# df6, for 2007
print(df6_2007_norm['export_value'].mean())
print(df6_2007_norm['export_value'].var())
df6_2007_norm['export_value'].hist(bins=10)

# df2, for 2007
n_countries = len(df2_2007_norm['location_id'].unique())
n_countries

# df4, for 2007
n_countries = len(df4_2007_norm['location_id'].unique())
n_countries

# df6, for 2007
n_countries = len(df6_2007_norm['location_id'].unique())
n_countries

# for df2, for 2007
n_products = len(df2_2007_norm['product_id'].unique())
n_products

# for df4, for 2007
n_products = len(df4_2007_norm['product_id'].unique())
n_products

# for df6, for 2007
n_products = len(df6_2007_norm['product_id'].unique())
n_products

"""## Create Dot Product Model - Simple Shallow Learning"""

# Creating product embedding path
product_input = Input(shape=[1], name='Product_Input')
product_embedding = Embedding(n_products+1, 5, name='Product_Embedding')(product_input)
product_vec = Flatten(name='Flatten-Products')(product_embedding)
print(product_input, product_embedding, product_vec)

# Creating country embedding path
country_input = Input(shape=[1], name='Country-Input')
country_embedding = Embedding(n_countries+1, 5, name='Country_Embedding')(country_input)
country_vec = Flatten(name='Flatten-Countries')(country_embedding)
print(country_input, country_embedding, country_vec)

# Performing dot product and creating model; can change decay to 1e-6
prod = Dot(name='Dot_Product', axes=1)([product_vec, country_vec])
model_dot = Model([country_input, product_input], prod)
adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False, clipnorm=1)
model_dot.compile(optimizer=adam, loss='mean_squared_error', metrics=['cosine_proximity'])

# Run the dot product model
if os.path.exists('regression_model_dot.h5'):
  model_dot = load_model('regression_model_dot.h5')
else:
  history = model_dot.fit([df6_2007_norm.location_id, df6_2007_norm.product_id], df6_2007_norm.export_value, batch_size=128, epochs=5, verbose=1)
  model_dot.save('regression_model_dot')
  plt.plot(history.history['loss'])
  plt.xlabel('Epochs')
  plt.ylabel('Training Error')
  
# regularization loss - l2 - worth forcing the weights to stay small; some weight is going

"""https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network

Loss probably not working because of exploding gradient problem?
"""

# Evaluate model_dot on 2008
model_dot.evaluate([df6_2008_norm.location_id, df6_2008_norm.product_id], df6_2008_norm.export_value)

# Make predictions using model_dot - probably need to un-normalize from minmax
predictions_dot = model_dot.predict([df6_2008_norm.location_id.head(10), df6_2008_norm.product_id.head(10)])

# Denormalize
for i in range(0,10):
    predictions_dot[i] = predictions_dot[i]*(df6_2008['export_value'].max()-df6_2008['export_value'].min())+df6_2008['export_value'].min()

predictions_dot

# Compare predictions with actual
[print(predictions_dot[i], df6_2008.export_value.iloc[i]) for i in range(0,10)]

'''
# Evaluate model_dot on 2017
model_dot.evaluate([df6_2017_norm.location_id, df6_2017_norm.product_id], df6_2017_norm.export_value)
'''

'''
# Make predictions using model_dot - probably need to un-normalize from minmax
predictions_dot = model_dot.predict([df6_2017_norm.location_id.head(10), df6_2017_norm.product_id.head(10)])

# Denormalize
for i in range(0,10):
    predictions_dot[i] = predictions_dot[i]*(df6_2017['export_value'].max()-df6_2017['export_value'].min())+df6_2017['export_value'].min()

predictions_dot

# Compare predictions with actual
[print(predictions_dot[i], df6_2017.export_value.iloc[i]) for i in range(0,10)]
'''

df6_2008.head()

"""## Creating Neural Network"""

# Creating product embedding path
product_input = Input(shape=[1], name='Product-Input')
product_embedding = Embedding(n_products+1, 5, name='Product-Embedding')(product_input)
product_vec = Flatten(name='Flatten-Products')(product_embedding)

# Creating country embedding path
country_input = Input(shape=[1], name='Country-Input')
country_embedding = Embedding(n_countries+1, 5, name='Country-Embedding')(country_input)
country_vec = Flatten(name='Flatten-Countries')(country_embedding)

'''
# Compile model
# can add regularization or dropout? kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01) - leads to excessively slow learning/very high loss

# too many 0's with relu activation - try tanh or LeakyReLU(0.3); softmax for probability

# Concatenate features
conc = Concatenate()([product_vec, country_vec])

# Add fully-connected layers
fc1 = Dense(128)(conc)
fc2 = advanced_activations.LeakyReLU(alpha=0.3)(fc1)
fc3 = Dense(32)(fc2)
fc4 = advanced_activations.LeakyReLU(alpha=0.3)(fc3)
out = Dense(1)(fc4)

# Create model and compile it
model_nn = Model([country_input, product_input], out)
model_nn.compile('adam', 'mean_squared_error', metrics=['cosine_proximity'])
'''

# Compile model
# can add regularization or dropout? kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01) - leads to excessively slow learning/very high loss

# too many 0's with relu activation - try tanh or LeakyReLU(0.3); softmax for probability

# Concatenate features
conc = Concatenate()([product_vec, country_vec])

# Add fully-connected layers
fc1 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))(conc)
fc2 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))(fc1)
out = Dense(1)(fc2)

# Create model and compile it
model_nn = Model([country_input, product_input], out)
model_nn.compile('adam', 'mean_squared_error', metrics=['cosine_proximity'])

# Run the NN model
if os.path.exists('regression_model_nn.h5'):
  model_nn = load_model('regression_model_nn.h5')
else:
  history = model_nn.fit([df6_2007_norm.location_id, df6_2007_norm.product_id], df6_2007_norm.export_value, epochs=5, verbose=1)
  model_nn.save('regression_model_nn.h5')
  plt.plot(history.history['loss'])
  plt.xlabel('Number of Epochs')
  plt.ylabel('Training Error')

# Evaluate model_nn on 2008
model_nn.evaluate([df6_2008_norm.location_id, df6_2008_norm.product_id], df6_2008_norm.export_value)

# Make predictions using model_nn
predictions_nn = model_nn.predict([df6_2008_norm.location_id.head(10), df6_2008_norm.product_id.head(10)])

# Denormalize
for i in range(0,10):
    predictions_nn[i] = predictions_nn[i]*(df6_2008['export_value'].max()-df6_2008['export_value'].min())+df6_2008['export_value'].min()

predictions_nn

# Compare predictions with actual
[print(predictions_nn[i], df6_2008.export_value.iloc[i]) for i in range(0,10)]

'''
# Evaluate model_nn on 2017
model_nn.evaluate([df6_2017_norm.location_id, df6_2017_norm.product_id], df6_2017_norm.export_value)
'''

'''
# Make predictions using model_nn
predictions_nn = model_nn.predict([df6_2017_norm.location_id.head(10), df6_2017_norm.product_id.head(10)])

# Denormalize
for i in range(0,10):
    predictions_nn[i] = predictions_nn[i]*(df6_2017['export_value'].max()-df6_2017['export_value'].min())+df6_2017['export_value'].min()

predictions_nn

# Compare predictions with actual
[print(predictions_nn[i], df6_2017.export_value.iloc[i]) for i in range(0,10)]
'''

"""#Visualizing Embeddings
Embeddings are weights that are learned to represent some specific variable like products and countries in our case and, therefore, we can not only use them to get good results on our problem but also extract insight about our data.
"""

# Extract embeddings
product_em = model_nn.get_layer('Product-Embedding')
product_em_weights = product_em.get_weights()[0]

product_em_weights[:5]

pca = PCA(n_components=2)
pca_result = pca.fit_transform(product_em_weights)
sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1])

product_em_weights = product_em_weights / np.linalg.norm(product_em_weights, axis=1).reshape((-1,1))
product_em_weights[0][:10]
np.sum(np.square(product_em_weights[0]))

pca = PCA(n_components=2)
pca_result = pca.fit_transform(product_em_weights)
sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1])

tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tnse_results = tsne.fit_transform(product_em_weights)

sns.scatterplot(x=tnse_results[:,0], y=tnse_results[:,1])

"""#Making Recommendations"""

len(df6_2007_norm.product_id)

# Creating dataset for making recommendations for the first country
product_data = np.array(list(set(df6_2007_norm.product_id)))
product_data

len(product_data)

df6_2007_norm.loc[df6_2007_norm['product_id']==8192].head()

country = np.array([1 for i in range(len(product_data))])
country[:5]

# show normalized prediction values
predictions = model_nn.predict([country, product_data])
predictions

predictions = np.array([a[0] for a in predictions])
predictions

# denormalize prediction values
for i in range(len(predictions)):
    predictions[i] = predictions[i]*(df6_2007['export_value'].max()-df6_2007['export_value'].min())+df6_2007['export_value'].min()

predictions

len(predictions)
predictions.min()

# show recommended products (i.e. top export values)
recommended_product_ids = (-predictions).argsort()[:5]
recommended_product_ids

# print predicted export_value - normalized
predictions[recommended_product_ids]

# show predicted product details - first all products
df6_classes.head()

df_classes[df_classes['index'].isin(recommended_product_ids)]

df6_2008_norm.loc[df6_2008_norm['product_id']==3366]

df6_2008_norm.loc[8366:8380]



# Make predictions using model_nn
inference = model_nn.predict([df6_2008_norm.location_id.head(10), df6_2008_norm.product_id.head(10)])

# Denormalize
for i in range(0,10):
    inference[i] = inference[i]*(df6_2008['export_value'].max()-df6_2008['export_value'].min())+df6_2008['export_value'].min()

inference

# Compare predictions with actual
[print(inference[i], df6_2008.export_value.iloc[i]) for i in range(0,10)]

recommended_product_ids = (-inference).argsort()[:10]
recommended_product_ids

