{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Faran Sikandar\n",
    "Insight AI.SV19B\n",
    "06/08/2019\n",
    "\n",
    "Project: Net_Align\n",
    "Description: Using Representation Learning to Improve Recommender Systems for Economic Diversification\n",
    "\n",
    "Data: Atlas of Economic Complexity\n",
    "\n",
    "Notes:\n",
    "- Recommender system code inspired from https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb\n",
    "\n",
    "Based off of original Colab file:\n",
    "\n",
    "rec_sys_simple.ipynb\n",
    "\n",
    "https://colab.research.google.com/drive/1P64VIbq6-FWVKYo503NT4y-_GgaIilgD\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"# Imports and Setup\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check relevant TF, keras, and GPU connections\n",
    "\n",
    "# show which version of TF working\n",
    "!pip show tensorflow\n",
    "\n",
    "# show which version of keras\n",
    "!pip show keras\n",
    "\n",
    "'''\n",
    "# check GPU connection\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import chain\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import advanced_activations, Concatenate, Dense, Dot, Dropout, Embedding, Flatten, Input, LSTM, Reshape\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Setup Google Drive + Get the Data\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "# mount Google Drive locally\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "'''\n",
    "\n",
    "# load paths\n",
    "def load_paths(gdrive = False, ide = False):\n",
    "    '''\n",
    "    input: choose whether you are running the script in gdrive, as a shell, or locally (e.g. ide)\n",
    "    output: relative paths for directories, hdf_filename || can add other directories and files as needed\n",
    "    '''\n",
    "    cwd = os.getcwd()\n",
    "    cwd\n",
    "    # for shell\n",
    "    try:\n",
    "        directory = os.path.dirname(os.path.abspath(__file__))\n",
    "        hdf_filename = os.path.join(directory,'data/raw/data_17_0.h5')\n",
    "    except NameError:\n",
    "        # for gdrive\n",
    "        if gdrive == True:\n",
    "            hdf_filename = '/content/gdrive/My Drive/Insight_AI/Insight_Net_Align/data/raw/data_17_0.h5'\n",
    "        # for local (e.g. IDE)\n",
    "        if ide == True:\n",
    "            directory = os.path.dirname(os.path.abspath('data_17_0.h5'))\n",
    "            directory\n",
    "            hdf_filename = os.path.join(directory,'data/raw/data_17_0.h5')\n",
    "            hdf_filename\n",
    "    dict_paths_def = {'cwd':cwd, 'directory':directory, 'hdf_filename':hdf_filename}\n",
    "    return dict_paths_def\n",
    "\n",
    "dict_paths = load_paths(ide = True)\n",
    "\n",
    "# check path names\n",
    "dict_paths\n",
    "\n",
    "# load the data\n",
    "def load_data():\n",
    "    hdf_def = pd.HDFStore(dict_paths['hdf_filename'], mode='r')\n",
    "    return hdf_def\n",
    "\n",
    "hdf = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preview keys in HDF\n",
    "hdf.keys()\n",
    "\n",
    "# extract country summaries\n",
    "df_country = hdf.get('/country')\n",
    "print(df_country.shape)\n",
    "df_country.head()\n",
    "\n",
    "# extract country hs product lookbacks\n",
    "df2_lookback = hdf.get('/country_hsproduct2digit_lookback')\n",
    "df4_lookback = hdf.get('/country_hsproduct4digit_lookback') # there is no 6digit lookback\n",
    "df4_lookback.head()\n",
    "\n",
    "# extract country summaries lookback\n",
    "df_country_lookback = hdf.get('/country_lookback')\n",
    "df_country_lookback.tail()\n",
    "\n",
    "# compare country + lookback shapes\n",
    "print(df_country.shape)\n",
    "print(df_country_lookback.shape)\n",
    "\n",
    "# look at just one set of lookbacks for a given year\n",
    "df_country_lookback3 = df_country_lookback[df_country_lookback['lookback_year'] == 2014]\n",
    "print(df_country_lookback3.shape)\n",
    "\n",
    "# extract classes - index is the product_id in other tables\n",
    "df_classes = hdf.get('/classifications/hs_product')\n",
    "type(df_classes)\n",
    "print(df_classes.shape)\n",
    "df2_classes = df_classes[df_classes['level'] == '2digit']\n",
    "df4_classes = df_classes[df_classes['level'] == '4digit']\n",
    "df6_classes = df_classes[df_classes['level'] == '6digit']\n",
    "\n",
    "print(df6_classes.shape)\n",
    "df6_classes.head()\n",
    "\n",
    "# get locations - index is location_id in other tables\n",
    "df_locations = hdf.get('/classifications/location')\n",
    "print(df_locations.shape)\n",
    "df_locations.head()\n",
    "\n",
    "# extract hs product data\n",
    "df2 = hdf.get('/country_hsproduct2digit_year')\n",
    "df4 = hdf.get('/country_hsproduct4digit_year')\n",
    "df6 = hdf.get('/country_hsproduct6digit_year')\n",
    "\n",
    "print(df6.shape)\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# clean the NaNs\n",
    "try:\n",
    "    df2 = df2.fillna(0)\n",
    "except:\n",
    "    df2\n",
    "\n",
    "df2\n",
    "\n",
    "df4 = df4.fillna(0)\n",
    "df6 = df6.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# df6.groupby(['location_id','year']).sum().reset_index() # if don't do reset_index(), then loc and year b/c part of index - does all columns\n",
    "\n",
    "def group_sum(df, groups, targets, reset_index = True):\n",
    "    '''\n",
    "    input:  data = pandas df to groupby sum\n",
    "            groups = list of features to groupbym e.g. ['location_id','year']\n",
    "            targets = list variables to sum e.g. ['export_value']\n",
    "    output: groupby sum\n",
    "    '''\n",
    "    if reset_index == True:\n",
    "        df_groupsum = df.groupby(groups)[targets].sum().reset_index() # can also do .agg('sum')\n",
    "    else:\n",
    "        df_groupsum = df.groupby(groups)[targets].sum()\n",
    "    return df_groupsum\n",
    "\n",
    "\n",
    "# sum the exports/imports, by location and year - will be used for improved normalization by country\n",
    "df6_groupsum = ( group_sum(df=df6,groups=['location_id','year'],targets=['export_value','import_value'])\n",
    ".rename(index=str, columns={'export_value':'export_total', 'import_value':'import_total'}) )\n",
    "\n",
    "df6_groupsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def data_filter(df,filter,values):\n",
    "    '''\n",
    "    input:  df: pandas df to cut\n",
    "            filter: single var to filter by e.g. year\n",
    "            value: list or any iterable of filter value(s)\n",
    "    output: df filtered by value\n",
    "    '''\n",
    "    df_filter = df.loc[df[filter].isin(values)]\n",
    "    return df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data for a 10 year TRAIN range\n",
    "df6_95_04 = data_filter(df=df6,filter='year',values=range(1995,2005))\n",
    "df6_95_04.head(10)\n",
    "\n",
    "# filter the data for a 10 year TEST range\n",
    "df6_05_14 = data_filter(df=df6,filter='year',values=range(2005,2015))\n",
    "df6_05_14.head(10)\n",
    "\n",
    "# TRAIN sum the exports/imports across the FIRST half of the time slice - for trend analysis\n",
    "df6_95_04_sum1 = ( df6_95_04.loc[df6_95_04['year'].isin(range(1995,2000))].groupby(['location_id','product_id'])['export_value','import_value']\n",
    ".sum().reset_index().rename(index=str, columns={'export_value':'export_period1', 'import_value':'import_period1'}) )\n",
    "\n",
    "df6_95_04_sum1\n",
    "\n",
    "# TRAIN sum the exports/imports across the SECOND half of the time slice - for trend analysis\n",
    "df6_95_04_sum2 = ( df6_95_04.loc[df6_95_04['year'].isin(range(2000,2005))].groupby(['location_id','product_id'])['export_value','import_value']\n",
    ".sum().reset_index().rename(index=str, columns={'export_value':'export_period2', 'import_value':'import_period2'}) )\n",
    "\n",
    "df6_95_04_sum2\n",
    "\n",
    "# TEST sum the exports/imports across the FIRST half of the time slice - for trend analysis\n",
    "df6_05_14_sum1 = ( df6_05_14.loc[df6_05_14['year'].isin(range(2005,2015))].groupby(['location_id','product_id'])['export_value','import_value']\n",
    ".sum().reset_index().rename(index=str, columns={'export_value':'export_period1', 'import_value':'import_period1'}) )\n",
    "\n",
    "df6_05_14_sum1\n",
    "\n",
    "# TEST sum the exports/imports across the SECOND half of the time slice - for trend analysis\n",
    "df6_05_14_sum2 = ( df6_05_14.loc[df6_05_14['year'].isin(range(2005,2015))].groupby(['location_id','product_id'])['export_value','import_value']\n",
    ".sum().reset_index().rename(index=str, columns={'export_value':'export_period2', 'import_value':'import_period2'}) )\n",
    "\n",
    "df6_05_14_sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and merge sum2 and export/import trends back into sum1 df; fill NaNs with 0 (if 0 base value)\n",
    "df6_95_04_trend = ( df6_95_04_sum1.assign( export_period2 = df6_95_04_sum2['export_period2'], import_period2 = df6_95_04_sum2['import_period2'],\n",
    "export_trend = lambda x: ((x.export_period2 - df6_95_04_sum1['export_period1'])/x.export_period2).fillna(0),\n",
    "import_trend = lambda x: ((x.import_period2 - df6_95_04_sum1['import_period1'])/x.import_period2).fillna(0) ) )\n",
    "\n",
    "df6_05_14_trend = ( df6_05_14_sum1.assign( export_period2 = df6_05_14_sum2['export_period2'], import_period2 = df6_05_14_sum2['import_period2'],\n",
    "export_trend = lambda x: ((x.export_period2 - df6_05_14_sum1['export_period1'])/x.export_period2).fillna(0),\n",
    "import_trend = lambda x: ((x.import_period2 - df6_05_14_sum1['import_period1'])/x.import_period2).fillna(0) ) )\n",
    "\n",
    "# how to use assign to create multiple values in df\n",
    "# df = df.assign(Val10_minus_Val1 = df['Val10'] - df['Val1'], log_result = lambda x: np.log(x.Val10_minus_Val1) )\n",
    "\n",
    "df6_95_04_trend\n",
    "df6_05_14_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# impute export inf/-inf with max/min trend for 95_04\n",
    "mask_pos = df6_95_04_trend['export_trend'] != np.inf\n",
    "mask_pos\n",
    "mask_neg = df6_95_04_trend['export_trend'] != -np.inf\n",
    "mask_neg\n",
    "df6_95_04_trend[~mask_neg]\n",
    "\n",
    "df6_95_04_trend.loc[~mask_pos, 'export_trend'] = df6_95_04_trend.loc[mask_pos, 'export_trend'].max()\n",
    "df6_95_04_trend.loc[~mask_neg, 'export_trend'] = df6_95_04_trend.loc[mask_neg, 'export_trend'].min()\n",
    "\n",
    "# impute export inf/-inf with max/min trend for 05_14\n",
    "mask_pos = df6_05_14_trend['export_trend'] != np.inf\n",
    "mask_pos\n",
    "mask_neg = df6_05_14_trend['export_trend'] != -np.inf\n",
    "mask_neg\n",
    "df6_05_14_trend[~mask_neg]\n",
    "\n",
    "df6_05_14_trend.loc[~mask_pos, 'export_trend'] = df6_05_14_trend.loc[mask_pos, 'export_trend'].max()\n",
    "df6_05_14_trend.loc[~mask_neg, 'export_trend'] = df6_05_14_trend.loc[mask_neg, 'export_trend'].min()\n",
    "\n",
    "# impute import inf/-inf with max/min trend for 95_04\n",
    "mask_pos = df6_95_04_trend['import_trend'] != np.inf\n",
    "mask_pos\n",
    "mask_neg = df6_95_04_trend['import_trend'] != -np.inf\n",
    "mask_neg\n",
    "df6_95_04_trend[~mask_neg]\n",
    "\n",
    "df6_95_04_trend.loc[~mask_pos, 'import_trend'] = df6_95_04_trend.loc[mask_pos, 'import_trend'].max()\n",
    "df6_95_04_trend.loc[~mask_neg, 'import_trend'] = df6_95_04_trend.loc[mask_neg, 'import_trend'].min()\n",
    "\n",
    "# impute import inf/-inf with max/min trend for 05_14\n",
    "mask_pos = df6_05_14_trend['import_trend'] != np.inf\n",
    "mask_pos\n",
    "mask_neg = df6_05_14_trend['import_trend'] != -np.inf\n",
    "mask_neg\n",
    "df6_05_14_trend[~mask_neg]\n",
    "\n",
    "df6_05_14_trend.loc[~mask_pos, 'import_trend'] = df6_05_14_trend.loc[mask_pos, 'import_trend'].max()\n",
    "df6_05_14_trend.loc[~mask_neg, 'import_trend'] = df6_05_14_trend.loc[mask_neg, 'import_trend'].min()\n",
    "\n",
    "df6_95_04_trend.describe()\n",
    "df6_95_04_trend\n",
    "\n",
    "# can think about normalizing export trend according to overall export volume during period average for each country or for each product\n",
    "# df6_95_04_trend.groupby(['location_id'])['export_period1'].sum()\n",
    "\n",
    "# pd.DataFrame(df.values*df2.values, columns=df.columns, index=df.index)\n",
    "\n",
    "# df6_95_04_trend.apply(lambda x:x.replace([-np.inf],x.min())).head(20)\n",
    "\n",
    "# for index, row in df6_95_04_trend.iterrows():\n",
    "#     if row['export_trend'] == np.nan:\n",
    "#         row['export_trend'] = 0\n",
    "#     elif row['export_trend'] == np.inf:\n",
    "#         row['export_trend'] = df6_95_04['export_trend'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df6_95_04_trend back into d56_95_04 by location and product (will be repeats of summed values)\n",
    "train = pd.merge(df6_95_04, df6_groupsum, on=['location_id','year'], how='inner')\n",
    "train = pd.merge(train, df6_95_04_trend, on=['location_id','product_id'], how='inner')\n",
    "print(train.shape)\n",
    "train\n",
    "\n",
    "# merge df6_05_14_trend back into d56_05_14 by location and product (will be repeats of summed values)\n",
    "test = pd.merge(df6_95_04, df6_groupsum, on=['location_id','year'], how='inner')\n",
    "test = pd.merge(test, df6_95_04_trend, on=['location_id','product_id'], how='inner')\n",
    "print(test.shape)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize exports on total exports of country by year\n",
    "cols = ['location_id','product_id','year','export_value','export_total','export_period1','export_period2','export_trend']\n",
    "train = train.copy(deep=True)\n",
    "train = train[cols]\n",
    "train\n",
    "\n",
    "test = test[cols]\n",
    "test = test.copy(deep=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate product percent of total exports\n",
    "train['export_pct'] = (train['export_value']/train['export_total'])\n",
    "train.head()\n",
    "\n",
    "test['export_pct'] = (test['export_value']/test['export_total'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# normalize by country and year - this may be redundant since we already made export_pct\n",
    "def norm_minmax(data,targets):\n",
    "    return (data[targets]-data[targets].min())/(data[targets].max()-data[targets].min())\n",
    "\n",
    "def norm_std(data,targets):\n",
    "    return (data[targets]-data[targets].mean())/(data[targets].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# norm across all countries and years\n",
    "train['export_pct_norm_all'] = norm_minmax(data=train,targets=['export_pct'])\n",
    "train['export_pct_std_all'] = norm_std(data=train,targets=['export_pct'])\n",
    "train.describe()\n",
    "train\n",
    "\n",
    "test['export_pct_norm_all'] = norm_minmax(data=test,targets=['export_pct'])\n",
    "test['export_pct_std_all'] = norm_std(data=test,targets=['export_pct'])\n",
    "test.describe()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize by country and year ??? doesn't seem to get me what I want - possible that you don't WANT to normalize by country and year, because perhaps overall global trade of goods is more important\n",
    "'''\n",
    "try:\n",
    "    start = time.perf_counter()\n",
    "    temp = train.groupby(['location_id','year']).apply(norm_minmax, targets='export_pct').to_frame().reset_index().rename('export_pct':'export_pct_norm')\n",
    "    end = time.perf_counter()\n",
    "finally:\n",
    "    print('run time: ', end-start)\n",
    "    temp\n",
    "'''\n",
    "\n",
    "train_pct_norm = ( train.groupby(['location_id','year']).apply(norm_minmax, targets='export_pct').to_frame()\n",
    ".rename(index=str, columns={'export_pct':'export_pct_norm'}).reset_index() )\n",
    "\n",
    "train_pct_std = ( train.groupby(['location_id','year']).apply(norm_std, targets='export_pct').to_frame()\n",
    ".rename(index=str, columns={'export_pct':'export_pct_std'}).reset_index() )\n",
    "\n",
    "train_trend_std = ( train.groupby(['location_id']).apply(norm_std, targets='export_trend').to_frame()\n",
    ".rename(index=str, columns={'export_trend':'export_trend_std'}).reset_index() )\n",
    "\n",
    "test_pct_norm = ( test.groupby(['location_id','year']).apply(norm_minmax, targets='export_pct').to_frame()\n",
    ".rename(index=str, columns={'export_pct':'export_pct_norm'}).reset_index() )\n",
    "\n",
    "test_pct_std = ( test.groupby(['location_id','year']).apply(norm_std, targets='export_pct').to_frame()\n",
    ".rename(index=str, columns={'export_pct':'export_pct_std'}).reset_index() )\n",
    "\n",
    "test_trend_std = ( test.groupby(['location_id']).apply(norm_std, targets='export_trend').to_frame()\n",
    ".rename(index=str, columns={'export_trend':'export_trend_std'}).reset_index() )\n",
    "\n",
    "'''\n",
    "# same as\n",
    "# df6_.groupby(['location_id','year']).apply( lambda x: (x['export_pct']-x['export_pct'].min())/(x['export_pct'].max()-x['export_pct'].min()) )\n",
    "# do product_id as well - otherwise indices lost?\n",
    "# df6_train.groupby(['location_id','year','product_id']).apply(norm_minmax, targets='export_pct').to_frame().reset_index()\n",
    "\n",
    "# df6_train.groupby(['location_id','year'])( (df6['export_pct']-df6['export_pct'].min())/(df6_train['export_pct'].max()-df6_train['export_pct'].min()) )\n",
    "# df2_2007_norm['export_value'] = (df2_2007['export_value']-df2_2007['export_value'].min())/(df2_2007['export_value'].max()-df2_2007['export_value'].min())\n",
    "# df6.groupby(['location_id','year'])['export_value'].sum().reset_index()\n",
    "'''\n",
    "\n",
    "train_pct_norm\n",
    "train_trend_std\n",
    "train_trend_std.describe()\n",
    "train_pct_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge the pct and trend norms in\n",
    "\n",
    "train_temp = train.join([train_pct_norm['export_pct_norm'], train_pct_std['export_pct_std'], train_trend_std['export_trend_std']])\n",
    "#train_temp = pd.merge(train, train_pct_norm['export_pct_norm'], train_pct_std['export_pct_std'], train_trend_norm['export_trend_norm'], left_index=True, right_index=True)\n",
    "train = train_temp\n",
    "train\n",
    "\n",
    "test_temp = test.join([test_pct_norm['export_pct_norm'], test_pct_std['export_pct_std'], test_trend_std['export_trend_std']])\n",
    "#test_temp = pd.merge(test, test_pct_norm['export_pct_norm'], train_pct_std['export_pct_std'], test_trend_norm['export_trend_norm'], left_index=True, right_index=True)\n",
    "test = test_temp\n",
    "test\n",
    "\n",
    "#df6_train['export_pct_norm'] = (df6_train['export_pct']-df6_train['export_pct'].min())/(df6_train['export_pct'].max()-df6_train['export_pct'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Export data to HDF5 and pickle\n",
    "\n",
    "# export to HDF5\n",
    "\n",
    "clean = ( {'train':train, 'test':test, 'df_country':df_country, 'df4_lookback':df4_lookback, 'df_country_lookback':df_country_lookback,\n",
    "'df6_classes':df6_classes, 'df_locations':df_locations} )\n",
    "\n",
    "for key, value in clean.items():\n",
    "    print(key)\n",
    "\n",
    "# always make train the first item in the dict\n",
    "for k, v in clean.items():\n",
    "    try:\n",
    "        if k == 'train':\n",
    "            v.to_hdf('data/processed/data_clean.h5', key=k, mode='w')\n",
    "        else:\n",
    "            v.to_hdf('data/processed/data_clean.h5', key=k)\n",
    "    except NotImplementedError:\n",
    "        if k == 'train':\n",
    "            v.to_hdf('data/processed/data_clean.h5', key=k, mode='w', format='t')\n",
    "        else:\n",
    "            v.to_hdf('data/processed/data_clean.h5', key=k, format='t')\n",
    "\n",
    "data_clean = pd.HDFStore('data/processed/data_clean.h5', mode='r')\n",
    "data_clean.keys()\n",
    "\n",
    "#pd.read_hdf('data.h5')\n",
    "train = data_clean.get('/train')\n",
    "test = data_clean.get('/test')\n",
    "\n",
    "train.head()\n",
    "test.head()\n",
    "\n",
    "data_clean.close()\n",
    "\n",
    "prep = ( {'df6':df6, 'df6_groupsum':df6_groupsum, 'df6_95_04':df6_95_04, 'df6_05_14':df6_05_14, 'df6_95_04_sum1':df6_95_04_sum1,\n",
    "'df6_95_04_sum2':df6_95_04_sum2, 'df6_95_04_trend':df6_95_04_trend, 'df6_05_14_sum1':df6_05_14_sum1, 'df6_05_14_sum2':df6_05_14_sum2,\n",
    "'df6_05_14_trend':df6_05_14_trend, 'train_pct_norm':train_pct_norm, 'train_pct_std':train_pct_std, 'train_trend_std':train_trend_std,\n",
    "'test_pct_norm':test_pct_norm, 'test_pct_std':test_pct_std, 'test_trend_std':test_trend_std} )\n",
    "\n",
    "for k, v in prep.items():\n",
    "    try:\n",
    "        if k == 'train':\n",
    "            v.to_hdf('data/preprocessed/data_prep.h5', key=k, mode='w')\n",
    "        else:\n",
    "            v.to_hdf('data/preprocessed/data_prep.h5', key=k)\n",
    "    except NotImplementedError:\n",
    "        if k == 'train':\n",
    "            v.to_hdf('data/preprocessed/data_prep.h5', key=k, mode='w', format='t')\n",
    "        else:\n",
    "            v.to_hdf('data/preprocessed/data_prep.h5', key=k, format='t')\n",
    "\n",
    "data_prep = pd.HDFStore('data/preprocessed/data_prep.h5', mode='r')\n",
    "data_prep.keys()\n",
    "data_prep.close()\n",
    "\n",
    "\n",
    "'''\n",
    "train.to_hdf('data_clean.h5', key='train', mode='w')\n",
    "test.to_hdf('data_clean.h5', key='test')\n",
    "df_country.to_hdf('data_clean.h5', key='df_country', format='t')\n",
    "df4_lookback.to_hdf('data_clean.h5', key='df4_lookback', format='t')\n",
    "df_country_lookback.to_hdf('data_clean.h5', key='df_country_lookback', format='t')\n",
    "df6_classes.to_hdf('data_clean.h5', key='df6_classes')\n",
    "df_locations.to_hdf('data_clean.h5', key='df_locations')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "\n",
    "train.describe()\n",
    "\n",
    "\n",
    "mask_pos = df6_95_04_trend['export_trend'] != np.inf\n",
    "mask_pos\n",
    "mask_neg = df6_95_04_trend['export_trend'] != -np.inf\n",
    "mask_neg\n",
    "df6_95_04_trend[~mask_neg]\n",
    "\n",
    "train['export_value'].hist(bins=10)\n",
    "mask = train['export_pct'] == 0\n",
    "train[~mask]['export_pct'].hist(bins=10)\n",
    "\n",
    "\n",
    "'''\n",
    "#rng = np.random.RandomState(10)  # deterministic random data\n",
    "rng = train[~mask]['export_value']\n",
    "#a = np.hstack((rng.normal(size=1000),\n",
    "#                rng.normal(loc=5, scale=2, size=1000)))\n",
    "plt.hist(rng, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "# df6, for 2007\n",
    "n_countries = len(train['location_id'].unique())\n",
    "n_countries\n",
    "\n",
    "# for df6, for 2007\n",
    "n_products = len(train['product_id'].unique())\n",
    "n_products\n",
    "\n",
    "n_latent_factors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Create Dot Product Model - Simple Shallow Learning\"\"\"\n",
    "\n",
    "# Creating product embedding path\n",
    "product_input = Input(shape=[1], name='Product_Input')\n",
    "product_embedding = Embedding(n_products+1, n_latent_factors, name='Product_Embedding')(product_input)\n",
    "product_vec = Flatten(name='Flatten-Products')(product_embedding)\n",
    "print(product_input, product_embedding, product_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating country embedding path\n",
    "country_input = Input(shape=[1], name='Country-Input')\n",
    "country_embedding = Embedding(n_countries+1, n_latent_factors, name='Country_Embedding')(country_input)\n",
    "country_vec = Flatten(name='Flatten-Countries')(country_embedding)\n",
    "print(country_input, country_embedding, country_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing dot product and creating model; can change decay to 1e-6\n",
    "prod = Dot(name='Dot_Product', axes=1)([product_vec, country_vec])\n",
    "model_dot = Model([country_input, product_input], prod)\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False, clipnorm=1)\n",
    "model_dot.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error','cosine_proximity'])\n",
    "model_dot.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dot product model\n",
    "if os.path.exists('models/regression_model_dot.h5'):\n",
    "  model_dot = load_model('models/regression_model_dot.h5')\n",
    "else:\n",
    "  history_dot = model_dot.fit([train.location_id, train.product_id], train.export_pct, batch_size=128, epochs=5, verbose=1)\n",
    "  model_dot.save('models/regression_model_dot')\n",
    "  plt.plot(history_dot.history['loss'])\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Training Error')\n",
    "\n",
    "# regularization loss - l2 - worth forcing the weights to stay small; some weight is going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network\n",
    "\n",
    "Loss probably not working because of exploding gradient problem?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model_dot on 2008\n",
    "model_dot.evaluate([test.location_id, test.product_id], test.export_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using model_dot - probably need to un-normalize from minmax\n",
    "predictions_dot = model_dot.predict([test.location_id.head(10), test.product_id.head(10)])\n",
    "\n",
    "# Denormalize - different denormalization needed depending on target used\n",
    "# for i in range(0,10):\n",
    "#     predictions_dot[i] = predictions_dot[i]*(test['export_value'].max()-test['export_value'].min())+test['export_value'].min()\n",
    "\n",
    "predictions_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions with actual\n",
    "[print(predictions_dot[i], test.export_pct.iloc[i]) for i in range(0,10)]\n",
    "\n",
    "'''\n",
    "# Evaluate model_dot on 2017\n",
    "model_dot.evaluate([df6_2017_norm.location_id, df6_2017_norm.product_id], df6_2017_norm.export_value)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Make predictions using model_dot - probably need to un-normalize from minmax\n",
    "predictions_dot = model_dot.predict([df6_2017_norm.location_id.head(10), df6_2017_norm.product_id.head(10)])\n",
    "\n",
    "# Denormalize\n",
    "for i in range(0,10):\n",
    "    predictions_dot[i] = predictions_dot[i]*(df6_2017['export_value'].max()-df6_2017['export_value'].min())+df6_2017['export_value'].min()\n",
    "\n",
    "predictions_dot\n",
    "\n",
    "# Compare predictions with actual\n",
    "[print(predictions_dot[i], df6_2017.export_value.iloc[i]) for i in range(0,10)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Creating Neural Network\"\"\"\n",
    "\n",
    "# Creating product embedding path\n",
    "product_input = Input(shape=[1], name='Product-Input')\n",
    "product_embedding = Embedding(n_products+1, n_latent_factors, name='Product-Embedding')(product_input)\n",
    "product_vec = Flatten(name='Flatten-Products')(product_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating country embedding path\n",
    "country_input = Input(shape=[1], name='Country-Input')\n",
    "country_embedding = Embedding(n_countries+1, n_latent_factors, name='Country-Embedding')(country_input)\n",
    "country_vec = Flatten(name='Flatten-Countries')(country_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Compile model\n",
    "# can add regularization or dropout? kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01) - leads to excessively slow learning/very high loss\n",
    "\n",
    "# too many 0's with relu activation - try tanh or LeakyReLU(0.3); softmax for probability\n",
    "\n",
    "# Concatenate features\n",
    "conc = Concatenate()([product_vec, country_vec])\n",
    "\n",
    "# Add fully-connected layers\n",
    "fc1 = Dense(128)(conc)\n",
    "fc2 = advanced_activations.LeakyReLU(alpha=0.3)(fc1)\n",
    "fc3 = Dense(32)(fc2)\n",
    "fc4 = advanced_activations.LeakyReLU(alpha=0.3)(fc3)\n",
    "out = Dense(1)(fc4)\n",
    "\n",
    "# Create model and compile it\n",
    "model_nn = Model([country_input, product_input], out)\n",
    "model_nn.compile('adam', 'mean_squared_error', metrics=['cosine_proximity'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# can add regularization or dropout? kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01) - leads to excessively slow learning/very high loss\n",
    "\n",
    "# too many 0's with relu activation - try tanh or LeakyReLU(0.3); softmax for probability\n",
    "\n",
    "# Concatenate features\n",
    "conc = Concatenate()([product_vec, country_vec])\n",
    "\n",
    "# Add fully-connected layers\n",
    "fc1 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))(conc)\n",
    "fc2 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))(fc1)\n",
    "out = Dense(1)(fc2)\n",
    "\n",
    "# Create model and compile it\n",
    "model_nn = Model([country_input, product_input], out)\n",
    "model_nn.compile('adam', 'mean_squared_error', metrics=['mean_squared_error','cosine_proximity'])\n",
    "model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NN model\n",
    "if os.path.exists('models/regression_model_nn.h5'):\n",
    "  model_nn = load_model('models/regression_model_nn.h5')\n",
    "else:\n",
    "  history_nn = model_nn.fit([train.location_id, train.product_id], train.export_pct, epochs=5, verbose=1)\n",
    "  model_nn.save('models/regression_model_nn.h5')\n",
    "  plt.plot(history_nn.history['loss'])\n",
    "  plt.xlabel('Number of Epochs')\n",
    "  plt.ylabel('Training Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model_nn on 2008\n",
    "model_nn.evaluate([test.location_id, test.product_id], test.export_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using model_nn\n",
    "predictions_nn = model_nn.predict([test.location_id.head(10), test.product_id.head(10)])\n",
    "\n",
    "# Denormalize - different denormalization needed depending on target used\n",
    "# for i in range(0,10):\n",
    "#     predictions_nn[i] = predictions_nn[i]*(test['export_value'].max()-test['export_value'].min())+test['export_value'].min()\n",
    "\n",
    "predictions_nn\n",
    "\n",
    "# Compare predictions with actual\n",
    "[print(predictions_nn[i], test.export_pct.iloc[i]) for i in range(0,10)]\n",
    "\n",
    "'''\n",
    "# Evaluate model_nn on 2017\n",
    "model_nn.evaluate([df6_2017_norm.location_id, df6_2017_norm.product_id], df6_2017_norm.export_value)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Make predictions using model_nn\n",
    "predictions_nn = model_nn.predict([df6_2017_norm.location_id.head(10), df6_2017_norm.product_id.head(10)])\n",
    "\n",
    "# Denormalize\n",
    "for i in range(0,10):\n",
    "    predictions_nn[i] = predictions_nn[i]*(df6_2017['export_value'].max()-df6_2017['export_value'].min())+df6_2017['export_value'].min()\n",
    "\n",
    "predictions_nn\n",
    "\n",
    "# Compare predictions with actual\n",
    "[print(predictions_nn[i], df6_2017.export_value.iloc[i]) for i in range(0,10)]\n",
    "'''\n",
    "\n",
    "\"\"\"#Visualizing Embeddings\n",
    "Embeddings are weights that are learned to represent some specific variable like products and countries in our case and, therefore, we can not only use them to get good results on our problem but also extract insight about our data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "product_em = model_nn.get_layer('Product-Embedding')\n",
    "product_em_weights = product_em.get_weights()[0]\n",
    "\n",
    "product_em_weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(product_em_weights)\n",
    "sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_em_weights = product_em_weights / np.linalg.norm(product_em_weights, axis=1).reshape((-1,1))\n",
    "product_em_weights[0][:10]\n",
    "np.sum(np.square(product_em_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(product_em_weights)\n",
    "sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tnse_results = tsne.fit_transform(product_em_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=tnse_results[:,0], y=tnse_results[:,1])\n",
    "\n",
    "\"\"\"#Making Recommendations\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.product_id)\n",
    "\n",
    "# Creating dataset for making recommendations for the first country\n",
    "product_data = np.array(list(set(train.product_id)))\n",
    "product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(product_data)\n",
    "\n",
    "train.loc[train['product_id']==8192].head()\n",
    "\n",
    "country = np.array([1 for i in range(len(product_data))])\n",
    "country[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show normalized prediction values\n",
    "predictions = model_nn.predict([country, product_data])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([a[0] for a in predictions])\n",
    "predictions\n",
    "\n",
    "# denormalize prediction values - different needed\n",
    "# for i in range(len(predictions)):\n",
    "#     predictions[i] = predictions[i]*(df6_2007['export_value'].max()-df6_2007['export_value'].min())+df6_2007['export_value'].min()\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)\n",
    "predictions.min()\n",
    "\n",
    "# show recommended products (i.e. top export values)\n",
    "recommended_product_ids = (-predictions).argsort()[:5]\n",
    "recommended_product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print predicted export_value - normalized\n",
    "predictions[recommended_product_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show predicted product details - first all products\n",
    "df6_classes = df6_classes.reset_index()\n",
    "df6_classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6_classes[df6_classes['index'].isin(recommended_product_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['product_id']==3366]\n",
    "\n",
    "test.loc[8366:8380]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using model_nn\n",
    "inference = model_nn.predict([test.location_id.head(10), test.product_id.head(10)])\n",
    "\n",
    "# Denormalize\n",
    "# for i in range(0,10):\n",
    "#     inference[i] = inference[i]*(df6_2008['export_value'].max()-df6_2008['export_value'].min())+df6_2008['export_value'].min()\n",
    "\n",
    "inference\n",
    "\n",
    "# Compare predictions with actual\n",
    "[print(inference[i], test.export_value.iloc[i]) for i in range(0,10)]\n",
    "\n",
    "recommended_product_ids2 = (-inference).argsort()[:10]\n",
    "recommended_product_ids2"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
